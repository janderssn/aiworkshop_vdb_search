
pip3 install torch torchvision torchaudio ~250mb
pip3 install pillow
pip3 install chromadb
pip3 install ftfy regex tqdm
pip3 install git+https://github.com/openai/CLIP.git

Embedding model
we're using OpenAIs clip 


 The clip.load function initializes the CLIP model and its preprocessing utilities. ViT-B/32 specifies the variant of CLIP you're loading, which uses a Vision Transformer architecture.
```
model, preprocess = clip.load("ViT-B/32", device=device)
```


```python
# Import the clip module from the CLIP library and the device management from PyTorch
import clip
from torch import device

# Load the CLIP model and its preprocessing pipeline. "ViT-B/32" specifies the model architecture and version. 
# 'device' specifies the hardware (CPU/GPU) on which the model will run.
model, preprocess = clip.load("ViT-B/32", device=device)

# Define a class method to encode images.
def encode_image(self, image):
    # Preprocess the image (resize, crop, normalize) and add a batch dimension so it can be processed by the model.
    image = self.preprocess(image).unsqueeze(0).to(self.device)
    # Disable gradient calculations to save memory and computations since we're only doing inference.
    with torch.no_grad():
        # Encode the preprocessed image to get the image features (embeddings).
        image_features = self.model.encode_image(image)
        # Extract the first (and only) item from the features to get the embedding vector.
        image_embeddings = image_features[0]
        # Convert the tensor of embeddings to a Python list for easier handling.
        image_embeddings: list = list(image_embeddings.numpy())
        # Print a success message with the type of the image_embeddings variable.
        print(f"Successfully encoded and converted {type(image_embeddings)}")
    # Return the list of image embeddings.
    return image_embeddings

# Define a function to encode an image given its file path.
def encode_image_from_path(self, path):
    # Open the image file from the given path.
    image = Image.open(path)
    # Use the encode_image method to encode this image and return the embeddings.
    return self.encode_image(image)

```